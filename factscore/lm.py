import pickle
import os
import time


class LM(object):

    def __init__(self, cache_file):
        self.cache_file = cache_file
        self.cache_dict = None
        self.cache_dict = self.load_cache()
        self.model = None
        self.add_n = 0

    def load_model(self):
        # load the model and put it as self.model
        raise NotImplementedError()

    def generate(
        self, prompt, sample_idx=0, max_sequence_length=2048, max_output_length=128
    ):
        prompt = prompt.strip()  # it's important not to end with a whitespace
        cache_key = f"{prompt}_{sample_idx}"

        if cache_key in self.cache_dict:
            return self.cache_dict[cache_key]

        if self.model is None:
            self.load_model()

        if prompt.endswith(" True or False?\nAnswer:"):
            generated = self._generate(
                prompt, max_sequence_length=max_sequence_length, max_output_length=1
            )
        else:
            generated = self._generate(
                prompt,
                max_sequence_length=max_sequence_length,
                max_output_length=max_output_length,
            )

        self.cache_dict[cache_key] = generated
        self.add_n += 1
        return generated

    def save_cache(self):
        if self.add_n == 0:
            return

        # load the latest cache first, since if there were other processes running in parallel, cache might have been updated
        for k, v in self.load_cache().items():
            self.cache_dict[k] = v

        with open(self.cache_file, "wb") as f:
            pickle.dump(self.cache_dict, f)

    def load_cache(self):
        if self.cache_dict is not None:
            if os.path.exists(self.cache_file):
                with open(self.cache_file, "rb") as f:
                    cache = pickle.load(f)
            else:
                cache = self.cache_dict
        else:
            cache = {}
        return cache
